{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "* $\\textbf{Motivation}$: The electrochemical oxygen evolution reaction (OER) is one of the most important reactions in renewable energy technology. It is an anode reaction of water splitting to produce H$_2$ and O$_2$, and it can be coupled with cathodic reactions such as CO$_2$ or N$_2$ reduction reactions to produce valuable chemicals under ambient conditions, which otherwise requires energy-intensive processes. The current challenge of water-splitting reaction is to discover better catalysts in a large chemical search space. Since water-splitting reaction is electro-chemical reaction, one of the catalyst selection criteria is to find materials that are conductive (i.e. with low bandgap). The cost of generating bandgap data and large chemical space inspired adopting accelerated computational screenings, such as machine learning. \n",
    "\n",
    "* $\\textbf{Objective}$:The goal of this project is to develop a sequential learning procedure (SLP) that selects  materials suitable for water splitting reaction. The selection criterion would be the badgap of the materials. \n",
    "\n",
    "* $\\textbf{Dataset}$: The dataset used in the research presented in Stein et al. Chem. Sci., 2019, 10, 47-55 https://doi.org/10.1039/C8SC03077D. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from monty.os import cd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold, train_test_split, learning_curve, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "from camd.agent.base import HypothesisAgent\n",
    "from camd.analysis import AnalyzerBase\n",
    "from camd.experiment.base import ATFSampler\n",
    "from camd.campaigns.base import Campaign\n",
    "\n",
    "from agents_helper_scripts import get_features_from_df, BandgapAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load data\n",
    "Download files to SageMaker instance from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jcap_pickle = 'jcap_optical_encoding.pickle'\n",
    "energy_npy = 'energy_ev.npy'\n",
    "\n",
    "def hook(t):\n",
    "    def inner(bytes_amount):\n",
    "        t.update(bytes_amount)\n",
    "    return inner\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "for filename in [jcap_pickle, energy_npy]:\n",
    "    filesize = boto3.resource('s3').Object('hackathon2020-prod', 'data/' + filename).content_length\n",
    "    with tqdm(total=filesize, unit='B', unit_scale=True, desc=jcap_pickle) as t:\n",
    "        s3.download_file('hackathon2020-prod', 'data/' + filename, filename, Callback=hook(t))\n",
    "        \n",
    "energy_ev = np.load(energy_npy)\n",
    "jcap_df = pd.read_pickle(jcap_pickle)\n",
    "print('This dataset has {} samples, {} features'.format(jcap_df.shape[0], jcap_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Parition data into seed and candidate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random state=42 allows us to reproduce same dfs each time\n",
    "seed_orig_df, candidate_orig_df = train_test_split(jcap_df, test_size=0.9, random_state=42)\n",
    "\n",
    "# make cop of seed, candidate df so we don't modify jcap_df \n",
    "seed_df = seed_orig_df.copy()\n",
    "candidate_df = candidate_orig_df.copy()\n",
    "\n",
    "print('{} seed data, {} candidates to explore'.format(len(seed_df), len(candidate_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_df['bandgap'].plot.hist(bins=40,range=[0,3])\n",
    "plt.xlabel('bandgap [eV]')\n",
    "plt.savefig('seed_bandgap_hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jcap_df['bandgap'].plot.hist(bins=40,range=[0,1.2])\n",
    "plt.xlabel('bandgap [eV]')\n",
    "plt.savefig('seed_bandgap_hist_zoom.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Test various different ML algorithms and pick the best ones.\n",
    "There are a lot of issues one has to consider before solving a machine learning problem. These issues includes data preparation, feature selection, feature engineering, model selection and validation, hyperparameter tuning, etc. In theory, you can find and apply a plethora of techniques for each of these components, but they all might perform differently for different datasets. The challenge is to find the best performing combination of techniques so that you can minimize the error in your machine learning workflow. \n",
    "\n",
    "In this step, we will outline a series of steps to choose the best machine learning model for our agent. We focused on model selection and hyperparameter tuning. If time and resources allow in the future, these steps should be refined to intelligently explore the possible ML models to find the most suitable one for your exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper funtion\n",
    "# -------------------------------------------------\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "def plot_learning_curve(estimator, X, y, cv=kf, n_jobs=4, train_sizes=np.linspace(.1, 1.0, 10), \n",
    "                        color=None, label=None):\n",
    "    \"\"\"\n",
    "    Function that generates a learning curve. The data are split into various train sizes.\n",
    "    At each train size, a 3-fold CV is used on training data, and validated on the validation data. \n",
    "    The mean and standard deviation of the CV error is plotted. \n",
    "    \n",
    "    Args:\n",
    "        estimator        ML model\n",
    "        X                list of features\n",
    "        y                list of labels      \n",
    "\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='neg_mean_absolute_error')\n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "    train_scores_std = -np.std(train_scores, axis=1)\n",
    "    test_scores_mean = -np.mean(test_scores, axis=1)\n",
    "    test_scores_std = -np.std(test_scores, axis=1)\n",
    "    plthandle, = plt.plot(train_sizes*1.5, test_scores_mean, 'o-', color=color,\n",
    "             label=label,ms=15)\n",
    "    plt.fill_between(train_sizes*1.5, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=plthandle.get_color())\n",
    "    print('Total time', time.time()-t0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 ML Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we chose linear regression, random forest regressor, and adaboost regressor. \n",
    "# Each model used sklearn default hyparparamaters for a baseline comparison\n",
    "lr = LinearRegression()\n",
    "rf = RandomForestRegressor(n_estimators=20, n_jobs=-1)\n",
    "ada = AdaBoostRegressor()\n",
    "\n",
    "# Process Data\n",
    "seed_features = get_features_from_df(seed_df)\n",
    "seed_labels = seed_df['bandgap']\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.grid()\n",
    "\n",
    "plot_learning_curve(lr, seed_features, seed_labels, label='Linear Regression')\n",
    "plot_learning_curve(rf, seed_features, seed_labels,  label='RandomForest Regressor')\n",
    "plot_learning_curve(ada, seed_features, seed_labels,  label='AdaBoost Regressor')\n",
    "\n",
    "plt.xlabel('# Seed Experiment Data',fontsize=20)\n",
    "plt.ylabel('Cross-Validation Mean Absolute Validation Error [eV]',fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "plt.legend(loc='upper right', fontsize='xx-large')\n",
    "plt.savefig('lr_rf_ada_learning_curves.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Thoughts}$:\n",
    "* RandomForest and Linear Regression performs better and have lower standard deviation.\n",
    "* RandomForest performs better as dataset decreases. \n",
    "* RandomForest has more tunnable hyperparameters that could improve the prediction accuracy.\n",
    "\n",
    "$\\textbf{We will use RandomForest Regressor in our Agent.}$\n",
    "\n",
    "$\\textbf{**Note}$: As the experimental dataset increases, we might need to re-evaluate model selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Optimize Hyperparamaters\n",
    "We will use sklearn GridSearchCV here. <br>\n",
    "$\\textbf{**Note}$: The GridsearchCV becomes time consuming if we optimize large sets of paramaters, and/or have a more complex model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(seed_features, seed_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestRegressor(n_jobs=-1)\n",
    "rf_params = {'n_estimators':[5, 10, 20], 'random_state':[None, 1], 'min_samples_split':[5, 10, 20]}\n",
    "rf_clf = GridSearchCV(rf, rf_params)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "best_rf = rf_clf.best_estimator_\n",
    "best_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_agent = BandgapAgent(best_rf, num=10, random=False, explore=0.5)\n",
    "hypotheses = bg_agent.get_hypotheses(candidate_df, seed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_atf_experiment = ATFSampler(dataframe=jcap_df)\n",
    "k_atf_experiment.submit(hypotheses)\n",
    "results = k_atf_experiment.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Analyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BandgapAnalyzer(AnalyzerBase): \n",
    "    def analyze(self, new_experimental_results, seed_data):\n",
    "        new_seed = pd.concat(\n",
    "            [seed_data, new_experimental_results],\n",
    "        axis=0)\n",
    "        # Create a summary\n",
    "        average_new_bandgap = new_experimental_results.bandgap.mean()\n",
    "        average_dataset_bandgap = new_seed.bandgap.mean()\n",
    "        new_result_ranks = new_seed.bandgap.rank(pct=True).loc[\n",
    "            new_experimental_results.index\n",
    "        ]\n",
    "        min_new_bandgap = new_experimental_results.bandgap.min()\n",
    "        min_dataset_bandgap = new_seed.bandgap.min()\n",
    "        summary = pd.DataFrame({\n",
    "            \"average_new_bandgap\": [average_new_bandgap],\n",
    "            \"average_dataset_bandgap\": [average_dataset_bandgap],\n",
    "            \"average_rank\": [new_result_ranks.mean()],\n",
    "            \"min_new_bandgap\": [min_new_bandgap],\n",
    "            \"min_dataset_bandgap\": [min_dataset_bandgap]\n",
    "        })\n",
    "        return summary, new_seed\n",
    "    \n",
    "k_analyzer = BandgapAnalyzer()\n",
    "summary, new_seed = k_analyzer.analyze(hypotheses, seed_df)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Define campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from monty.os import cd\n",
    "from camd.campaigns.base import Campaign\n",
    "# Set up folders\n",
    "os.system('rm -rf test')\n",
    "os.system('mkdir -p test')\n",
    "# Reinitialize experiment to clear history\n",
    "k_atf_experiment = ATFSampler(dataframe=jcap_df)\n",
    "with cd('test'):\n",
    "    campaign = Campaign(\n",
    "        candidate_data=candidate_df, \n",
    "        seed_data=seed_df,\n",
    "        agent=bg_agent,\n",
    "        experiment=k_atf_experiment,\n",
    "        analyzer=k_analyzer\n",
    "    )\n",
    "    campaign.auto_loop(initialize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull up some results\n",
    "history = pd.read_pickle('test/history.pickle')\n",
    "#visualize learning\n",
    "history.plot(subplots=True,figsize=(5,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMDD",
   "language": "python",
   "name": "amdd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
